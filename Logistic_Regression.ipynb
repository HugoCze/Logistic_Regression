{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8545a9b8",
   "metadata": {},
   "source": [
    "# Logistic Regression Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20ae3a2",
   "metadata": {},
   "source": [
    "Logistic Regression is on of the best ways to preform classyfication. As the perfect example we can use the binary classyfication. So not only the continious ones as we previously did. The function need to differenciate the binary-caterogical data. The function is Sigmoid aka Logistic. Takes in any value and outputs it to be between 0 and 1. Confusion Matrix that belongs to Logistic function present as a table with both poitive and negative prediction as columns and rows. We have the values of True Negative and True Postivie as the correct predictions. And we have our False Positive(Type I error) and False Negative(False II Error). We can easly calculate the accurancy by adding the total of True Postivie and Negative cases and divding it by the total number of cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "66aec3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ubuntu/Spark/spark-3.3.0-bin-hadoop3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cbffeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9723e533",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 07:48:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('mylogreg').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "45065793",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42f28eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 07:59:06 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "my_data = spark.read.format('libsvm').load('sample_libsvm_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b070626c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "|  1.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[151,152,153...|\n",
      "|  0.0|(692,[129,130,131...|\n",
      "|  1.0|(692,[158,159,160...|\n",
      "|  1.0|(692,[99,100,101,...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[127,128,129...|\n",
      "|  1.0|(692,[154,155,156...|\n",
      "|  0.0|(692,[153,154,155...|\n",
      "|  0.0|(692,[151,152,153...|\n",
      "|  1.0|(692,[129,130,131...|\n",
      "|  0.0|(692,[154,155,156...|\n",
      "|  1.0|(692,[150,151,152...|\n",
      "|  0.0|(692,[124,125,126...|\n",
      "|  0.0|(692,[152,153,154...|\n",
      "|  1.0|(692,[97,98,99,12...|\n",
      "|  1.0|(692,[124,125,126...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "my_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f2c86a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_log_reg_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bf69954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/13 08:00:24 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "22/09/13 08:00:24 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "22/09/13 08:00:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "22/09/13 08:00:24 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "22/09/13 08:00:24 WARN BlockManager: Block broadcast_19 could not be removed as it was not found on disk or in memory\n",
      "22/09/13 08:00:24 ERROR BlockManagerStorageEndpoint: Error in removing broadcast 19\n",
      "org.apache.spark.SparkException: Block broadcast_19 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:500)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1984)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1962)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1948)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/13 08:00:24 WARN BlockManagerMaster: Failed to remove broadcast 19 with removeFromMaster = true - Block broadcast_19 does not exist\n",
      "org.apache.spark.SparkException: Block broadcast_19 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:500)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1984)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1962)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1948)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "22/09/13 08:00:24 ERROR ContextCleaner: Error cleaning broadcast 19\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:301)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:195)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:351)\n",
      "\tat org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)\n",
      "\tat org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:79)\n",
      "\tat org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:256)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3(ContextCleaner.scala:204)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$3$adapted(ContextCleaner.scala:195)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:195)\n",
      "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1446)\n",
      "\tat org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:189)\n",
      "\tat org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:79)\n",
      "Caused by: org.apache.spark.SparkException: Block broadcast_19 does not exist\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.blockDoesNotExistError(SparkCoreErrors.scala:234)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.blockInfo(BlockInfoManager.scala:237)\n",
      "\tat org.apache.spark.storage.BlockInfoManager.removeBlock(BlockInfoManager.scala:500)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlockInternal(BlockManager.scala:1984)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBlock(BlockManager.scala:1962)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$removeBroadcast$3$adapted(BlockManager.scala:1948)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n",
      "\tat org.apache.spark.storage.BlockManager.removeBroadcast(BlockManager.scala:1948)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint$$anonfun$receiveAndReply$1.$anonfun$applyOrElse$4(BlockManagerStorageEndpoint.scala:69)\n",
      "\tat scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)\n",
      "\tat org.apache.spark.storage.BlockManagerStorageEndpoint.$anonfun$doAsync$1(BlockManagerStorageEndpoint.scala:89)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "fitted_logreg_model = my_log_reg_model.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b336c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_summary = fitted_logreg_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f024054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- rawPrediction: vector (nullable = true)\n",
      " |-- probability: vector (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_summary.predictions.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4323582d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[127,128,129...|[20.3777627514872...|[0.99999999858729...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-21.114014198868...|[6.76550380000472...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-23.743613234676...|[4.87842678716177...|       1.0|\n",
      "|  1.0|(692,[152,153,154...|[-19.192574012720...|[4.62137287298144...|       1.0|\n",
      "|  1.0|(692,[151,152,153...|[-20.125398874699...|[1.81823629113068...|       1.0|\n",
      "|  0.0|(692,[129,130,131...|[20.4890549504196...|[0.99999999873608...|       0.0|\n",
      "|  1.0|(692,[158,159,160...|[-21.082940212814...|[6.97903542823766...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-19.622713503550...|[3.00582577446132...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.1594863606582...|[0.99999999935352...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[28.1036706837287...|[0.99999999999937...|       0.0|\n",
      "|  1.0|(692,[154,155,156...|[-21.054076780106...|[7.18340962960324...|       1.0|\n",
      "|  0.0|(692,[153,154,155...|[26.9648490510184...|[0.99999999999805...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[32.7855654161400...|[0.99999999999999...|       0.0|\n",
      "|  1.0|(692,[129,130,131...|[-20.331839179667...|[1.47908944089721...|       1.0|\n",
      "|  0.0|(692,[154,155,156...|[21.7830579106564...|[0.99999999965347...|       0.0|\n",
      "|  1.0|(692,[150,151,152...|[-20.640562103728...|[1.08621994880353...|       1.0|\n",
      "|  0.0|(692,[124,125,126...|[22.6400775503731...|[0.99999999985292...|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[38.0712919910909...|           [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-19.830803265627...|[2.44113371545874...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-21.016054806036...|[7.46179590484091...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_summary.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f07949",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_train, lr_test = my_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8127cd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4b1e13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_final = final_model.fit(lr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2803bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_and_labes = fit_final.evaluate(lr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd949021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[122,123,148...|[22.7382598995413...|[0.99999999986667...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[34.6174435693185...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[29.4211013109438...|[0.99999999999983...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.0975759299945...|[0.99999999931223...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[37.5001506761958...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[16.2479181298831...|[0.99999991217487...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[24.0438036707843...|[0.99999999996386...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[32.0685602689225...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[28.7268718845501...|[0.99999999999966...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[16.3353890379520...|[0.99999991953062...|       0.0|\n",
      "|  0.0|(692,[155,156,180...|[37.0129274500828...|           [1.0,0.0]|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-13.256014301631...|[1.74978777043485...|       1.0|\n",
      "|  1.0|(692,[99,100,101,...|[-3.0094787884612...|[0.04699948551008...|       1.0|\n",
      "|  1.0|(692,[119,120,121...|[1.19920046235328...|[0.76838251966072...|       0.0|\n",
      "|  1.0|(692,[124,125,126...|[-23.520507416877...|[6.09780338797576...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-18.522456453307...|[9.03232107306500...|       1.0|\n",
      "|  1.0|(692,[124,125,126...|[-26.255092512975...|[3.95875112459718...|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-25.309140356486...|[1.01948303097656...|       1.0|\n",
      "|  1.0|(692,[126,127,128...|[-26.977984028533...|[1.92136733724572...|       1.0|\n",
      "|  1.0|(692,[127,128,129...|[-22.191612033938...|[2.30306051196632...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction_and_labes.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "723e4ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dbce60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval = BinaryClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d7f93cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_final_roc = my_eval.evaluate(prediction_and_labes.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73222e0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_final_roc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad4ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
